{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find html link raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_0 = 'Sent from my iPhone'\n",
    "urls = urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',url_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find all of text from html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"http://www.cnbc.com/id/101790001?__source=yahoo%7Cfinance%7Cheadline%7Cheadline%7Cstory&par=yahoo&doc=101790001%7CThink%20college%20is%20expensiv#\")\n",
    "soup = BeautifulSoup(r.content)\n",
    "text =[''.join(s.findAll(text=True))for s in soup.findAll('p')]\n",
    "print len(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Covert Chinese date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-10\n"
     ]
    }
   ],
   "source": [
    "t0 = u'2016年10月10日'\n",
    "s0 = re.findall(ur'(\\d+)年(\\d+)月(\\d+)日',t0)\n",
    "s1 = str(ss[0][0]) + '-' + str(ss[0][1]) + '-' + str(ss[0][2])\n",
    "print s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-14\n"
     ]
    }
   ],
   "source": [
    "#N days ago\n",
    "from datetime import datetime, timedelta\n",
    "date_N_days_ago = datetime.now() - timedelta(days=2)\n",
    "print date_N_days_ago.date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractResult(subdomain='mil', domain='stackoverflow', suffix='com.cn')\n"
     ]
    }
   ],
   "source": [
    "#find the domain and host\n",
    "import tldextract\n",
    "parsed_url = 'http://mil.stackoverflow.com.cn/questions/1234567/blah-blah-blah-blah'\n",
    "t0 = tldextract.extract(parsed_url)\n",
    "\n",
    "print t0\n",
    "\n",
    "df['host'] = df.url.map(lambda x: x.split('://')[1].split('/')[0] )  \n",
    "df['domain']  = df.url.map(lambda x:tldextract.extract(x).domain + str('.') + tldextract.extract(x).suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset encoding\n",
    "\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stanford NLP segmenter\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "segmenter = StanfordSegmenter(path_to_jar='/Users/nhu2000/desktop/nltk/stanford-segmenter-2014-08-27/stanford-segmenter-3.4.1.jar', \n",
    "                              path_to_slf4j='/Users/nhu2000/desktop/nltk/stanford-segmenter-2014-08-27/slf4j-api.jar',\n",
    "                              path_to_sihan_corpora_dict='/Users/nhu2000/desktop/nltk/stanford-segmenter-2014-08-27/data', \n",
    "                              path_to_model='/Users/nhu2000/desktop/nltk/stanford-segmenter-2014-08-27/data/pku.gz', \n",
    "                              path_to_dict='/Users/nhu2000/desktop/nltk/stanford-segmenter-2014-08-27/data/dict-chris6.ser.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop duplicates and NA\n",
    "df_no_dup = df_dropped.drop_duplicates(['title'])\n",
    "df_no_dup = df_no_dup.dropna(subset =['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loop through all files under given folder\n",
    "import glob\n",
    "all_files = glob.glob(\"./data/json/\" + \"/*.json\")\n",
    "\n",
    "#print all_files\n",
    "for file_ in all_files:\n",
    "    print file_\n",
    "    with open(file_) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        #print data\n",
    "        payload_headers = data['payload']['headers']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split pandas dataframe\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find value in rows\n",
    "df_0 = target_df[target_df['url'] == url]\n",
    "df_0.iloc[0]['segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find difference between two same clumns of data frames\n",
    "merged = target_df.merge(target_df, indicator=True, how='outer')\n",
    "\n",
    "merged_both = merged[merged['_merge'] == 'both']\n",
    "merged_rightonly = merged[merged['_merge'] == 'right_only']\n",
    "merged_leftonly = merged[merged['_merge'] == 'left_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get clean contents from html\n",
    "def html_get_content(html, cut = 0):\n",
    "    soup = BeautifulSoup(html.decode('utf-8'))\n",
    "    [script.extract() for script in soup.findAll('script')]\n",
    "    [style.extract() for style in soup.findAll('style')]\n",
    "    soup.prettify()\n",
    "    reg1 = re.compile(\"<[^>]*>\")\n",
    "    #content = soup\n",
    "    content = reg1.sub('',soup.prettify())\n",
    "    s = content.split('\\n')\n",
    "    ret = [a for a in s if len(a) > cut]\n",
    "    ret0 = '\\n'.join(ret)\n",
    "    #print ret0\n",
    "    return ret0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten a list of list\n",
    "ret = [item for sublist in words for item in sublist]\n",
    "flatten = lambda words: [item for sublist in words for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文化 徽州 的 ” “ 与 徽商 徽派 ： 徽文化\n"
     ]
    }
   ],
   "source": [
    "#find text inside quotation\n",
    "import re\n",
    "t = '-0.359*\"文化\" + -0.330*\"徽州\" + -0.321*\"的\" + -0.240*\"”\" + -0.240*\"“\" + -0.213*\"与\" + -0.206*\"徽商\" + -0.192*\"徽派\" + -0.140*\"：\" + -0.131*\"徽文化\"'\n",
    "s = re.findall('\"([^\"]*)\"', t)\n",
    "print ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all english letters, numbers, and special letters, = # & / \n",
    "match = re.compile('([a-zA-Z_0-9_=_#_&_/])') #remove letters, numbers, and special characters in the article body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
