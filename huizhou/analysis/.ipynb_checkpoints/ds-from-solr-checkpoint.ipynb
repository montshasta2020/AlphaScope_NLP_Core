{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nutch-crawl-huizhou-only2-01-15-2017-cleaned.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 64056 entries, 0 to 64055\n",
      "Data columns (total 7 columns):\n",
      "id         64056 non-null object\n",
      "tstamp     64056 non-null object\n",
      "url        64056 non-null object\n",
      "title      61836 non-null object\n",
      "digest     0 non-null float64\n",
      "content    0 non-null float64\n",
      "domain     64056 non-null object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_dup = df.drop_duplicates(['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38359 entries, 0 to 38363\n",
      "Data columns (total 12 columns):\n",
      "Unnamed: 0    38359 non-null int64\n",
      "_version_     38359 non-null int64\n",
      "cache         469 non-null object\n",
      "segment       38359 non-null int64\n",
      "digest        38359 non-null object\n",
      "tstamp        38359 non-null object\n",
      "url           38359 non-null object\n",
      "anchor        9833 non-null object\n",
      "content       37409 non-null object\n",
      "id            38359 non-null object\n",
      "title         36925 non-null object\n",
      "boost         38359 non-null float64\n",
      "dtypes: float64(1), int64(3), object(8)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_no_dup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_dup = df_no_dup.dropna(subset =['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = df_no_dup[['id', 'tstamp', 'url', 'title', 'digest', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37409 entries, 0 to 38363\n",
      "Data columns (total 6 columns):\n",
      "id         37409 non-null object\n",
      "tstamp     37409 non-null object\n",
      "url        37409 non-null object\n",
      "title      36900 non-null object\n",
      "digest     37409 non-null object\n",
      "content    37409 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhu2000/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_final['content_len'] = df_final['content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37409 entries, 0 to 38363\n",
      "Data columns (total 7 columns):\n",
      "id             37409 non-null object\n",
      "tstamp         37409 non-null object\n",
      "url            37409 non-null object\n",
      "title          36900 non-null object\n",
      "digest         37409 non-null object\n",
      "content        37409 non-null object\n",
      "content_len    37409 non-null int64\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>digest</th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://keylines.com/why</td>\n",
       "      <td>2016-01-01T16:45:57.898Z</td>\n",
       "      <td>http://keylines.com/why</td>\n",
       "      <td>Networks are everywhere</td>\n",
       "      <td>f07a49d7192aaac714ce450b297b4ef3</td>\n",
       "      <td>Networks are everywhere Blog Downloads Why Key...</td>\n",
       "      <td>3316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://keylines.com/working-with-us</td>\n",
       "      <td>2015-07-28T14:55:15.911Z</td>\n",
       "      <td>http://keylines.com/working-with-us</td>\n",
       "      <td>Working with us: Kick-start your project with ...</td>\n",
       "      <td>0c9db244dc6c0c9bd4b6233297f54bfe</td>\n",
       "      <td>Working with us: Kick-start your project with ...</td>\n",
       "      <td>3496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://keylines.com/wp-includes/wlwmanifest.xml</td>\n",
       "      <td>2016-01-01T16:45:55.807Z</td>\n",
       "      <td>http://keylines.com/wp-includes/wlwmanifest.xml</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dfd490b6f383ea02a269031ff05e8896</td>\n",
       "      <td>WordPress Yes Yes WordPress images/wlw/wp-icon...</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://keystone-ml.org/</td>\n",
       "      <td>2016-01-01T19:03:19.623Z</td>\n",
       "      <td>http://keystone-ml.org/</td>\n",
       "      <td>KeystoneML</td>\n",
       "      <td>42216a695f2df5d6e4aadf2402e2fc99</td>\n",
       "      <td>KeystoneML</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://killedbypolice.net/</td>\n",
       "      <td>2016-01-01T18:04:59.483Z</td>\n",
       "      <td>http://killedbypolice.net/</td>\n",
       "      <td>Killed By Police - 2015</td>\n",
       "      <td>164f9040f7b99811cee971626deb2b31</td>\n",
       "      <td>Killed By Police - 2015 Killed By Police 2015 ...</td>\n",
       "      <td>24322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                id                    tstamp  \\\n",
       "0                          http://keylines.com/why  2016-01-01T16:45:57.898Z   \n",
       "1              http://keylines.com/working-with-us  2015-07-28T14:55:15.911Z   \n",
       "2  http://keylines.com/wp-includes/wlwmanifest.xml  2016-01-01T16:45:55.807Z   \n",
       "3                          http://keystone-ml.org/  2016-01-01T19:03:19.623Z   \n",
       "4                       http://killedbypolice.net/  2016-01-01T18:04:59.483Z   \n",
       "\n",
       "                                               url  \\\n",
       "0                          http://keylines.com/why   \n",
       "1              http://keylines.com/working-with-us   \n",
       "2  http://keylines.com/wp-includes/wlwmanifest.xml   \n",
       "3                          http://keystone-ml.org/   \n",
       "4                       http://killedbypolice.net/   \n",
       "\n",
       "                                               title  \\\n",
       "0                            Networks are everywhere   \n",
       "1  Working with us: Kick-start your project with ...   \n",
       "2                                                NaN   \n",
       "3                                         KeystoneML   \n",
       "4                            Killed By Police - 2015   \n",
       "\n",
       "                             digest  \\\n",
       "0  f07a49d7192aaac714ce450b297b4ef3   \n",
       "1  0c9db244dc6c0c9bd4b6233297f54bfe   \n",
       "2  dfd490b6f383ea02a269031ff05e8896   \n",
       "3  42216a695f2df5d6e4aadf2402e2fc99   \n",
       "4  164f9040f7b99811cee971626deb2b31   \n",
       "\n",
       "                                             content  content_len  \n",
       "0  Networks are everywhere Blog Downloads Why Key...         3316  \n",
       "1  Working with us: Kick-start your project with ...         3496  \n",
       "2  WordPress Yes Yes WordPress images/wlw/wp-icon...          291  \n",
       "3                                         KeystoneML           10  \n",
       "4  Killed By Police - 2015 Killed By Police 2015 ...        24322  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['content_len'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhu2000/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_final['content'] = df_final['content'].str.encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189800810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['content_len'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from my_pickle import my_pickle_dump \n",
    "\n",
    "my_pickle_dump(\"ds_jan_2016_from_solr.pkl\", df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df_final['content'].values\n",
    "titles = df_final['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(chunks):\n",
    "#    print 'doc size', len(chunks)\n",
    "    words = [ tokenize.word_tokenize(sent) for sent in tokenize.sent_tokenize(chunks) ]\n",
    "    flatten = [ inner for sublist in words for inner in sublist ]\n",
    "    stripped = [] \n",
    "\n",
    "    for word in flatten:         \n",
    "        if word not in stopwords.words('english'):\n",
    "            try:\n",
    "                stripped.append(word.encode('latin-1').decode('utf8').lower())\n",
    "            except:\n",
    "                print \"Cannot encode: \" + word\n",
    "                pass\n",
    "    \n",
    "    s = [ word for word in stripped if len(word) > 1 ] \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnt = 0\n",
    "for doc in docs:\n",
    "    print cnt, len (s)\n",
    "    words = [ tokenize.word_tokenize(sent) for sent in tokenize.sent_tokenize(doc) ]\n",
    "    flatten = [ inner for sublist in words for inner in sublist ]\n",
    "    stripped = [] \n",
    "\n",
    "    for word in flatten:         \n",
    "        if word not in stopwords.words('english'):\n",
    "            try:\n",
    "                stripped.append(word.encode('latin-1').decode('utf8').lower())\n",
    "            except:\n",
    "                print \"Cannot encode: \" + word\n",
    "                pass\n",
    "    \n",
    "\n",
    "s = [ word for word in stripped if len(word) > 1 ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed = [ tokenize_and_normalize(s) for s in docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get parsed ...... 37409\n"
     ]
    }
   ],
   "source": [
    "print \"get parsed ......\", len(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pickle_dump(\"ds_parsed.pkl\", parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get dictionary ...... 412503\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(parsed)\n",
    "\n",
    "print \"get dictionary ......\", len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pickle_dump(\"ds_tfidf.pkl\", tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get tifid\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "print \"get tifid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pickle_dump(\"ds_corpus_tfidf.pkl\", corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get ldaModel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u\"0.012*thinkers + 0.006*best + 0.004*social + 0.004*world + 0.003*workforce + 0.003*analytics + 0.003*media + 0.003*'s + 0.002*smartdata + 0.002*data + 0.002*cloud + 0.002*followers + 0.002*collective + 0.002*management + 0.002*badge\"),\n",
       " (1,\n",
       "  u\"0.004*data + 0.002*big + 0.002*science + 0.002*analytics + 0.001*'' + 0.001*2015 + 0.001*... + 0.001*mining + 0.001*learning + 0.001*business + 0.001*sign + 0.001*news + 0.001*central + 0.001*blog + 0.001*predictive\"),\n",
       " (2,\n",
       "  u'0.003*bayesialab + 0.002*mariadb + 0.001*bayesia + 0.001*sql + 0.001*advizor + 0.001*keylines + 0.001*buffalo + 0.001*orleans + 0.001*bayesian + 0.001*nola.com + 0.001*dnp + 0.000*html + 0.000*connie + 0.000*fairfax + 0.000*videolectures.net'),\n",
       " (3,\n",
       "  u'0.001*maptitude + 0.001*spears + 0.001*charleston + 0.001*instructors + 0.001*winpure + 0.001*iit + 0.001*campus + 0.001*athletics + 0.001*wpi + 0.001*college + 0.001*liberal + 0.001*georgia + 0.001*sdiwc + 0.001*connecticut + 0.001*transmodeler'),\n",
       " (4,\n",
       "  u'0.002*dmu + 0.001*goldsmiths + 0.001*njit + 0.001*faststats + 0.001*dundee + 0.001*smu + 0.001*cooladata + 0.001*discoverer + 0.001*telerik + 0.000*apteco + 0.000*datadirect + 0.000*knoxville + 0.000*advancedminer + 0.000*clario + 0.000*peoplestage'),\n",
       " (5,\n",
       "  u'0.005*ibm + 0.004*meetups + 0.003*heroes + 0.003*chats + 0.003*developers + 0.003*animations + 0.003*blog + 0.002*bytes + 0.002*galleries + 0.002*hub + 0.002*streaming + 0.002*infographics + 0.002*podcasts + 0.002*big + 0.002*presentations'),\n",
       " (6,\n",
       "  u'0.001*und + 0.001*viagra + 0.001*der + 0.001*die + 0.001*cialis + 0.001*fr + 0.001*sie + 0.000*infoglide + 0.000*coventry + 0.000*datamentors + 0.000*levitra + 0.000*dysfunction + 0.000*erectile + 0.000*zu + 0.000*von'),\n",
       " (7,\n",
       "  u'0.002*angoss + 0.001*ssl + 0.001*techcrunch + 0.001*riskshield + 0.001*krems + 0.001*officereports + 0.001*tc + 0.001*atm + 0.001*medicine + 0.001*lindner + 0.000*forp + 0.000*pycon + 0.000*linux + 0.000*ev + 0.000*congress.gov'),\n",
       " (8,\n",
       "  u'0.004*de + 0.004*verified + 0.003*account + 0.003*tweets + 0.003*close + 0.002*retweet + 0.002*bahasa + 0.001*retweeted + 0.001*tweet + 0.001*twitter + 0.001*unblock + 0.001*blocked + 0.001*photos + 0.001*suggested + 0.001*embed'),\n",
       " (9,\n",
       "  u'0.001*adicio + 0.001*dealer + 0.001*invalid + 0.000*careercast.com + 0.000*motors + 0.000*classifieds + 0.000*turnkey + 0.000*trucking + 0.000*wraps + 0.000*faceted + 0.000*konect + 0.000*newspapers + 0.000*invoice + 0.000*edges + 0.000*promo'),\n",
       " (10,\n",
       "  u'0.002*blog-postapi-url + 0.001*anticipation + 0.001*wise + 0.001*practitioner + 0.001*iseg + 0.001*wordpress + 0.001*yes + 0.001*images/wlw/wp-icon.png + 0.001*/../wp-admin/post.php + 0.001*images/wlw/wp-comments.png + 0.001*action=edit + 0.001*/../wp-admin/edit-comments.php + 0.001*images/wlw/wp-watermark.png + 0.001*post= + 0.001*/../wp-admin/'),\n",
       " (11,\n",
       "  u'0.010*kdnuggets + 0.004*dec + 0.003*jun + 0.003*jul + 0.003*deep + 0.002*learning + 0.002*machine + 0.002*ftc + 0.002*stories + 0.001*nov + 0.001*empiricism + 0.001*triumph + 0.001*tweets + 0.001*50+ + 0.001*tags'),\n",
       " (12,\n",
       "  u'0.001*spss + 0.001*asiaanalytics + 0.001*re/code + 0.001*cedrus + 0.001*egon + 0.000*ibm + 0.000*fmri + 0.000*cmi + 0.000*pedal + 0.000*chronos + 0.000*foot + 0.000*pad + 0.000*genie + 0.000*lumina + 0.000*tw'),\n",
       " (13,\n",
       "  u'0.001*coursera + 0.001*walton + 0.001*enabled + 0.001*xtract + 0.001*mytools + 0.001*browsejobs + 0.001*managecoverletters + 0.001*resumesandcoverletters + 0.001*manageresumes + 0.000*xanalys + 0.000*hhmi + 0.000*imported + 0.000*analytictalent.com + 0.000*modern + 0.000*browse'),\n",
       " (14,\n",
       "  u'0.002*snhu + 0.001*art + 0.001*vedo + 0.001*datasift + 0.001*flytxt + 0.001*navik + 0.001*framed + 0.001*prints + 0.000*icon + 0.000*photography + 0.000*shop + 0.000*art.com + 0.000*canvas + 0.000*scivis + 0.000*altius')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda=LdaModel(corpus_tfidf, id2word=dictionary, num_topics=15, update_every=0, passes=200)\n",
    "\n",
    "print \"get ldaModel\"\n",
    "\n",
    "lda.print_topics(15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_pickle_dump(\"ds_lda.pkl\", lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = my_pickle_load(fname):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_n(vectorizer, vectors, data, n):\n",
    "    '''\n",
    "    Print out the top 10 words by three different sorting mechanisms:\n",
    "        * average tf-idf score\n",
    "        * total tf-idf score\n",
    "        * highest TF score across corpus\n",
    "    '''\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    # Top 10 words by average tfidf\n",
    "    # Take the average of each column, denoted by axis=0\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print \"top %d by average tf-idf\" % n\n",
    "    print get_top_values(avg, n, words)\n",
    "    print\n",
    "\n",
    "    # Top 10 words by total tfidf\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print \"top %d by total tf-idf\" % n\n",
    "    print get_top_values(total, n, words)\n",
    "    print\n",
    "\n",
    "    # Top 10 words by TF\n",
    "    vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "    # make documents into one giant document for this purpose\n",
    "    vectors2 = vectorizer2.fit_transform([\" \".join(data)]).toarray()\n",
    "    print \"top %d by tf across all corpus\" % n\n",
    "    print get_top_values(vectors2[0], n, words)\n",
    "    print\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values. Return\n",
    "    the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    ''' \n",
    "\n",
    "#     print \"get_top_values\", len(lst), n, len(labels)\n",
    "    \n",
    "#     seqs =  np.argsort(lst)[-1:-n-1:-1] \n",
    "#     for k in seqs:\n",
    "#         print k, labels[k]\n",
    "\n",
    "#    print 'before return 0000', len(seqs)\n",
    "        \n",
    "    ret = [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]\n",
    "\n",
    "#    print 'before return', len(ret)\n",
    "#     return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]\n",
    "    return ret\n",
    "\n",
    "def ranking(vectorizer, vectors, titles, queries, n):\n",
    "    '''\n",
    "    Print out the top n documents for each of the queries.\n",
    "    '''\n",
    "#    print queries\n",
    "    \n",
    "    tokenized_queries = vectorizer.transform(queries)\n",
    "    cosine_similarities = linear_kernel(tokenized_queries, vectors)\n",
    "    \n",
    "#    print 'cosine', cosine_similarities\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        print query\n",
    "        ret = get_top_values(cosine_similarities[i], 3, titles)\n",
    "        \n",
    "        #print \"ret len ---\", len(ret)\n",
    "        \n",
    "        for k in xrange(n):\n",
    "            print k, ret[k]\n",
    "#        print get_top_values(cosine_similarities[i], 3, titles)\n",
    "        print\n",
    "\n",
    "def get_queries(filename):\n",
    "    '''\n",
    "    Return a list of strings of the queries in the file.\n",
    "    '''\n",
    "    queries = []\n",
    "    with open('queries.txt') as f:\n",
    "        for line in f:\n",
    "            # horrible stuff to get out the query\n",
    "            ## Change made by Jack: line.split(\"   \")[1].split(\"20\")[0].strip() -> line\n",
    "            queries.append(line)\n",
    "    return queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_pickle import my_pickle_dump, my_pickle_load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_docs = [doc for doc in docs if len(doc) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37366\n"
     ]
    }
   ],
   "source": [
    "print len(cleaned_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = ['data science\\n', 'machine learning\\n', 'python\\n', 'deep learning\\n']\n",
    "\n",
    "# DO TFIDF TRANSFORMATION\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "print \"before fit\"\n",
    "# idx = 0\n",
    "# #vectors = my_pickle_load('ds_parsed.pkl')\n",
    "# for doc in cleaned_docs:\n",
    "#     print idx\n",
    "vectors = vectorizer.fit_transform(cleaned_docs).toarray()\n",
    "#     idx += 1\n",
    "\n",
    "print 'finished fit'\n",
    "my_pickle_dump(\"ds_vectorizer.pkl\", vectorizer)\n",
    "my_pickle_dump(\"ds_vectors.pkl\", vectors)\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "top_n(vectorizer, vectors, cleaned_docs, 10)\n",
    "\n",
    "# RANKING\n",
    "#ranking(vectorizer, vectors, pure_docs, get_queries('queries.txt'), 3)\n",
    "ranking(vectorizer, vectors, titles, queries, 3)\n",
    "\n",
    "\n",
    "# SUMMARIZATION\n",
    "#article = newsgroups.data[1599]  # can choose any article\n",
    "#summarization(article, categories, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(doc):\n",
    "    lowers = doc.lower()\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    \n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    \n",
    "    #tokens_no_stopwords = [token for token in tokens if token not in stopwords.word('english')] \n",
    "    #return tokens_no_stopwords\n",
    "    \n",
    "#    print (len(filtered))\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_tokens = [get_tokens(article) for article in docs]\n",
    "print len(tot_tokens)\n",
    "\n",
    "my_pickle_dump(\"ds_tot_tokens.pkl\", tot_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
