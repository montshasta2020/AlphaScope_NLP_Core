{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from my_pickle import my_pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"data/pkl/\" # NOTE: Update BASE_DIR to your own directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class TrainSentences(object):\n",
    "    \"\"\"\n",
    "    Iterator class that returns Sentences from texts files in a input directory\n",
    "    \"\"\"\n",
    "    RE_WIHTE_SPACES = re.compile(\"\\s+\")\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "    def __init__(self, dirname):\n",
    "        \"\"\"\n",
    "        Initialize a TrainSentences object with a input directory that contains text files for training\n",
    "        :param dirname: directory name which contains the text files        \n",
    "        \"\"\"\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Sentences iterator that return sentences parsed from files in the input directory.\n",
    "        Each sentences is returned as list of words\n",
    "        \"\"\"\n",
    "        #First iterate  on all files in the input directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # read line from file (Without reading the entire file)\n",
    "            for line in file(os.path.join(self.dirname, fname), \"rb\"):\n",
    "                # split the read line into sentences using NLTK\n",
    "                for s in txt2sentences(line, is_html=True):\n",
    "                    # split the sentence into words using regex\n",
    "                    w =txt2words(s, lower=True, is_html=False, remove_stop_words=False,\n",
    "                                                 remove_none_english_chars=True)\n",
    "                    #skip short sentneces with less than 3 words\n",
    "                    if len(w) < 3:\n",
    "                        continue\n",
    "                    yield w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainSentences(object):\n",
    "    \"\"\"\n",
    "    Iterator class that returns Sentences from texts files in a input directory\n",
    "    \"\"\"\n",
    "    RE_WIHTE_SPACES = re.compile(\"\\s+\")\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "    def __init__(self, dirname):\n",
    "        \"\"\"\n",
    "        Initialize a TrainSentences object with a input directory that contains text files for training\n",
    "        :param dirname: directory name which contains the text files        \n",
    "        \"\"\"\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Sentences iterator that return sentences parsed from files in the input directory.\n",
    "        Each sentences is returned as list of words\n",
    "        \"\"\"\n",
    "        #First iterate  on all files in the input directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # read line from file (Without reading the entire file)\n",
    "            fname_withpath = BASE_DIR + fname\n",
    "            df = my_pickle_load(fname_withpath)\n",
    "            \n",
    "            body_texts = df['body'].values\n",
    "            for body in body_texts:\n",
    "                # split the read line into sentences using NLTK\n",
    "                for s in txt2sentences(body, is_html=True):\n",
    "                    # split the sentence into words using regex\n",
    "                    w =txt2words(s, lower=True, is_html=False, remove_stop_words=False,\n",
    "                                                 remove_none_english_chars=True)\n",
    "                    #skip short sentneces with less than 3 words\n",
    "                    if len(w) < 3:\n",
    "                        continue\n",
    "                    yield w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt2sentences(txt, is_html=False, remove_none_english_chars=True):\n",
    "    \"\"\"\n",
    "    Split the English text into sentences using NLTK\n",
    "    :param txt: input text.\n",
    "    :param is_html: If True thenremove HTML tags using BeautifulSoup\n",
    "    :param remove_none_english_chars: if True then remove non-english chars from text\n",
    "    :return: string in which each line consists of single sentence from the original input text.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if is_html:\n",
    "        txt = BeautifulSoup(txt).get_text()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # split text into sentences using nltk packages\n",
    "    for s in tokenizer.tokenize(txt):\n",
    "        if remove_none_english_chars:\n",
    "            #remove none English chars\n",
    "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "        yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt2words(txt, lower=True, is_html=False, remove_none_english_chars=True, remove_stop_words=True):\n",
    "    \"\"\"\n",
    "    Split text into words list\n",
    "    :param txt: the input text\n",
    "    :param lower: if to make the  text to lowercase or not.\n",
    "    :param is_html: If True then  remove HTML tags using BeautifulSoup\n",
    "    :param remove_none_english_chars: if True then remove non-english chars from text\n",
    "    :param remove_stop_words: if True then remove stop words from text\n",
    "    :return: words list create from the input text according to the input parameters.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if is_html:\n",
    "        txt = BeautifulSoup(txt).get_text()\n",
    "    if lower:\n",
    "        txt = txt.lower()\n",
    "    if remove_none_english_chars:\n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "\n",
    "    words = TrainSentences.RE_WIHTE_SPACES.split(txt.strip().lower())\n",
    "    if remove_stop_words:\n",
    "        #remove stop words from text\n",
    "        words = [w for w in words if w not in TrainSentences.STOP_WORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = TrainSentences(\"%s\" % BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.TrainSentences'>\n"
     ]
    }
   ],
   "source": [
    "print type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhu2000/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, size=300, workers=8, min_count=40)\n",
    "model.save(\"%s/Dec_2015_DS_News.word2vec\" % BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'research', 0.6731033325195312),\n",
       " (u'technology', 0.6258788108825684),\n",
       " (u'global', 0.6027719974517822),\n",
       " (u'corporation', 0.593016505241394),\n",
       " (u'sciences', 0.5891018509864807),\n",
       " (u'institute', 0.5824580192565918),\n",
       " (u'center', 0.581402063369751),\n",
       " (u'massachusetts', 0.5739080905914307),\n",
       " (u'scripps', 0.5634201765060425),\n",
       " (u'division', 0.5609182119369507)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
