{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reuters-21578 text classification with Gensim and Keras\n",
    "#https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Numpy random seed\n",
    "import random\n",
    "random.seed(1000)\n",
    "\n",
    "# Newsline folder and format\n",
    "data_folder = '/Users/nhu2000/projects/SimilarSearch/keras/reuters21578/'\n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = {\n",
    "    'to_': ('Topics', 'all-topics-strings.lc.txt'),\n",
    "    'pl_': ('Places', 'all-places-strings.lc.txt'),\n",
    "    'pe_': ('People', 'all-people-strings.lc.txt'),\n",
    "    'or_': ('Organizations', 'all-orgs-strings.lc.txt'),\n",
    "    'ex_': ('Exchanges', 'all-exchanges-strings.lc.txt')\n",
    "}\n",
    "\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100\n",
    "# Selected categories\n",
    "selected_categories = ['pl_usa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare documents and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category dataframe\n",
    "\n",
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "for category_prefix in category_files.keys():\n",
    "    with open(data_folder + category_files[category_prefix][1], 'r') as file:\n",
    "        for category in file.readlines():\n",
    "            category_data.append([category_prefix + category.strip().lower(), \n",
    "                                  category_files[category_prefix][0], \n",
    "                                  0])\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = DataFrame(data=category_data, columns=['Name', 'Type', 'Newslines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_frequencies(categories):\n",
    "    for category in categories:\n",
    "        idx = news_categories[news_categories.Name == category].index[0]\n",
    "        f = news_categories.get_value(idx, 'Newslines')\n",
    "        news_categories.set_value(idx, 'Newslines', f+1)\n",
    "    \n",
    "def to_category_vector(categories, target_categories):\n",
    "    vector = np.zeros(len(target_categories)).astype(float)\n",
    "    \n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n",
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "# Parse SGML files\n",
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "\n",
    "# Iterate all files\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    \n",
    "    with open(data_folder + file_name, 'r') as file:\n",
    "        content = BeautifulSoup(file.read().lower())\n",
    "        \n",
    "        for newsline in content('reuters'):\n",
    "            document_categories = []\n",
    "            \n",
    "            # News-line Id\n",
    "            document_id = newsline['newid']\n",
    "            \n",
    "            # News-line text\n",
    "            document_body = strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')\n",
    "            document_body = unescape(document_body)\n",
    "            \n",
    "            # News-line categories\n",
    "            topics = newsline.topics.contents\n",
    "            places = newsline.places.contents\n",
    "            people = newsline.people.contents\n",
    "            orgs = newsline.orgs.contents\n",
    "            exchanges = newsline.exchanges.contents\n",
    "            \n",
    "            for topic in topics:\n",
    "                document_categories.append('to_' + strip_tags(str(topic)))\n",
    "                \n",
    "            for place in places:\n",
    "                document_categories.append('pl_' + strip_tags(str(place)))\n",
    "                \n",
    "            for person in people:\n",
    "                document_categories.append('pe_' + strip_tags(str(person)))\n",
    "                \n",
    "            for org in orgs:\n",
    "                document_categories.append('or_' + strip_tags(str(org)))\n",
    "                \n",
    "            for exchange in exchanges:\n",
    "                document_categories.append('ex_' + strip_tags(str(exchange)))\n",
    "                \n",
    "            # Create new document    \n",
    "            update_frequencies(document_categories)\n",
    "            \n",
    "            document_X[document_id] = document_body\n",
    "            document_Y[document_id] = to_category_vector(document_categories, selected_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Newslines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>pl_usa</td>\n",
       "      <td>Places</td>\n",
       "      <td>12542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>to_earn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>3987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>to_acq</td>\n",
       "      <td>Topics</td>\n",
       "      <td>2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>pl_uk</td>\n",
       "      <td>Places</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>pl_japan</td>\n",
       "      <td>Places</td>\n",
       "      <td>1138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pl_canada</td>\n",
       "      <td>Places</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>to_money-fx</td>\n",
       "      <td>Topics</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>to_crude</td>\n",
       "      <td>Topics</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>to_grain</td>\n",
       "      <td>Topics</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>pl_west-germany</td>\n",
       "      <td>Places</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>to_trade</td>\n",
       "      <td>Topics</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>to_interest</td>\n",
       "      <td>Topics</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>pl_france</td>\n",
       "      <td>Places</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>or_ec</td>\n",
       "      <td>Organizations</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pl_brazil</td>\n",
       "      <td>Places</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>to_wheat</td>\n",
       "      <td>Topics</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>to_ship</td>\n",
       "      <td>Topics</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pl_australia</td>\n",
       "      <td>Places</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>to_corn</td>\n",
       "      <td>Topics</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>pl_china</td>\n",
       "      <td>Places</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name           Type  Newslines\n",
       "161           pl_usa         Places      12542\n",
       "533          to_earn         Topics       3987\n",
       "498           to_acq         Topics       2448\n",
       "158            pl_uk         Places       1489\n",
       "84          pl_japan         Places       1138\n",
       "31         pl_canada         Places       1104\n",
       "571      to_money-fx         Topics        801\n",
       "526         to_crude         Topics        634\n",
       "543         to_grain         Topics        628\n",
       "167  pl_west-germany         Places        567\n",
       "624         to_trade         Topics        552\n",
       "553      to_interest         Topics        513\n",
       "56         pl_france         Places        469\n",
       "185            or_ec  Organizations        349\n",
       "23         pl_brazil         Places        332\n",
       "628         to_wheat         Topics        306\n",
       "606          to_ship         Topics        305\n",
       "10      pl_australia         Places        270\n",
       "517          to_corn         Topics        254\n",
       "37          pl_china         Places        223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 20 categories (by number of newslines)\n",
    "news_categories.sort_values(by='Newslines', ascending=False, inplace=True)\n",
    "news_categories.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize newsline documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenized document collection\n",
    "newsline_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "# Tokenize\n",
    "for key in document_X.keys():\n",
    "    newsline_documents.append(tokenize(document_X[key]))\n",
    "\n",
    "number_of_documents = len(document_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new Gensim Word2Vec model\n",
    "w2v_model = Word2Vec(newsline_documents, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing Word2Vec model\n",
    "w2v_model = Word2Vec.load(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorize each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(selected_categories)\n",
    "X = np.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(float)\n",
    "Y = np.zeros(shape=(number_of_documents, num_categories)).astype(float)\n",
    "\n",
    "empty_word = np.zeros(num_features).astype(float)\n",
    "\n",
    "for idx, document in enumerate(newsline_documents):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X[idx, jdx, :] = empty_word\n",
    "\n",
    "for idx, key in enumerate(document_Y.keys()):\n",
    "    Y[idx, :] = document_Y[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15104 samples, validate on 6474 samples\n",
      "Epoch 1/5\n",
      "15104/15104 [==============================] - 98s - loss: 0.6814 - acc: 0.5780 - val_loss: 0.6800 - val_acc: 0.5846\n",
      "Epoch 2/5\n",
      "15104/15104 [==============================] - 90s - loss: 0.6811 - acc: 0.5797 - val_loss: 0.6790 - val_acc: 0.5846\n",
      "Epoch 3/5\n",
      "15104/15104 [==============================] - 90s - loss: 0.6808 - acc: 0.5797 - val_loss: 0.6795 - val_acc: 0.5846\n",
      "Epoch 4/5\n",
      "15104/15104 [==============================] - 89s - loss: 0.6808 - acc: 0.5797 - val_loss: 0.6788 - val_acc: 0.5846\n",
      "Epoch 5/5\n",
      "15104/15104 [==============================] - 90s - loss: 0.6809 - acc: 0.5797 - val_loss: 0.6788 - val_acc: 0.5846\n",
      "6474/6474 [==============================] - 14s    \n",
      "Score: 0.6788\n",
      "Accuracy: 0.5846\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=5, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
