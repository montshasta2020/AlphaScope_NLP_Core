{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/solr-dsi-domain_only-04-05-2017-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='http://en.wikipedia.org/wiki/Automatic_summarization'\n",
    "parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = df['content'].str.contains('data science') \n",
    "df_check = df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple library and command line utility for extracting summary from HTML pages or plain texts. The package \n",
    "#also contains simple evaluation framework for text summaries. Implemented summarization methods:\n",
    "\n",
    "#    Luhn - heurestic method, reference\n",
    "#    Edmundson heurestic method with previous statistic research, reference\n",
    "#    Latent Semantic Analysis, LSA - one of the algorithm from http://scholar.google.com/citations?user=0fTuW_YAAAAJ&hl=en I think the author is using more advanced algorithms now. Steinberger, J. a JeĹľek, K. Using latent semantic an and summary evaluation. In In Proceedings ISIM ‘04. 2004. S. 93-100.\n",
    "#    LexRank - Unsupervised approach inspired by algorithms PageRank and HITS, reference\n",
    "#    TextRank - some sort of combination of a few resources that I found on the internet. I really don’t remember the sources. Probably Wikipedia and some papers in 1st page of Google :)\n",
    "#    SumBasic - Method that is often used as a baseline in the literature. Source: Read about SumBasic\n",
    "#    KL-Sum - Method that greedily adds sentences to a summary so long as it decreases the KL Divergence. Source: Read about KL-Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.127938032150269"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "for i in range(0, 100):  #8.591509103775024s\n",
    "    print (i, '------------------')\n",
    "    parser = HtmlParser.from_string(df_check.iloc[i]['content'], 'none', Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    #print (parser.document)\n",
    "    idx = 0\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            if (1): #idx != 0):\n",
    "                print(sentence)\n",
    "                print ('*********')\n",
    "            idx += 1\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "tr4s = TextRank4Sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.905581951141357"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time()\n",
    "from time import time\n",
    "t0 = time()\n",
    "for i in range(0, 100):  #11.573723077774048\n",
    "    print (i, '***********************')\n",
    "    tr4s.analyze(text=df_check.iloc[i]['content'], lower=True, source = 'all_filters')\n",
    "    for item in tr4s.get_key_sentences(num=3):\n",
    "        print(item.index, item.weight, item.sentence)  # index是语句在文本中位置，weight是权重\n",
    "        print ('------------------')\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8  \n",
    "import jieba  \n",
    "import networkx as nx  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer  \n",
    "  \n",
    "  \n",
    "def cut_sentence(sentence):  \n",
    "    \"\"\" \n",
    "    分句 \n",
    "    :param sentence: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    if not isinstance(sentence, unicode):  \n",
    "        sentence = sentence.decode('utf-8')  \n",
    "    delimiters = frozenset(u'。！？')  \n",
    "    buf = []  \n",
    "    for ch in sentence:  \n",
    "        buf.append(ch)  \n",
    "        if delimiters.__contains__(ch):  \n",
    "            yield ''.join(buf)  \n",
    "            buf = []  \n",
    "    if buf:  \n",
    "        yield ''.join(buf)  \n",
    "  \n",
    "  \n",
    "def load_stopwords(path='/home/fhqplzj/data/orion/stopwords.txt'):  \n",
    "    \"\"\" \n",
    "    加载停用词 \n",
    "    :param path: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    with open(path) as f:  \n",
    "        stopwords = filter(lambda x: x, map(lambda x: x.strip().decode('utf-8'), f.readlines()))  \n",
    "    stopwords.extend([' ', '\\t', '\\n'])  \n",
    "    return frozenset(stopwords)  \n",
    "  \n",
    "  \n",
    "def cut_words(sentence):  \n",
    "    \"\"\" \n",
    "    分词 \n",
    "    :param sentence: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    stopwords = load_stopwords()  \n",
    "    return filter(lambda x: not stopwords.__contains__(x), jieba.cut(sentence))  \n",
    "  \n",
    "  \n",
    "def get_abstract(content, size=3):  \n",
    "    \"\"\" \n",
    "    利用textrank提取摘要 \n",
    "    :param content: \n",
    "    :param size: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    docs = list(cut_sentence(content))  \n",
    "    \n",
    "    #tfidf_model = TfidfVectorizer(tokenizer=jieba.cut, stop_words=load_stopwords())  \n",
    "    tfidf_model = TfidfVectorizer()  \n",
    "    tfidf_matrix = tfidf_model.fit_transform(docs)  \n",
    "    normalized_matrix = TfidfTransformer().fit_transform(tfidf_matrix)  \n",
    "    similarity = nx.from_scipy_sparse_matrix(normalized_matrix * normalized_matrix.T)  \n",
    "    scores = nx.pagerank(similarity)  \n",
    "    tops = sorted(scores.iteritems(), key=lambda x: x[1], reverse=True)  \n",
    "    size = min(size, len(docs))  \n",
    "    indices = map(lambda x: x[0], tops)[:size]  \n",
    "    return map(lambda idx: docs[idx], indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = u'要说现在当红的90后男星，那就不得不提鹿晗、吴亦凡、杨洋、张艺兴、黄子韬、陈学冬、刘昊然，2016年他们带来不少人气爆棚的影视剧。这些90后男星不仅有颜值、有才华，还够努力，2017年他们又有哪些傲娇的作品呢？到底谁会成为2017霸屏男神，让我们拭目以待吧。鹿晗2016年参演《盗墓笔记》、《长城》、《摆渡人》等多部电影，2017年他将重心转到了电视剧。他和古力娜扎主演的古装奇幻电视剧《择天记》将在湖南卫视暑期档播出，这是鹿晗个人的首部电视剧，也是其第一次出演古装题材。该剧改编自猫腻的同名网络小说，讲述在人妖魔共存的架空世界里，陈长生(鹿晗饰演)为了逆天改命，带着一纸婚书来到神都，结识了一群志同道合的小伙伴，在国教学院打开一片新天地。吴亦凡在2017年有更多的作品推出。周星驰监制、徐克执导的春节档《西游伏魔篇》，吴亦凡扮演“有史以来最帅的”唐僧。师徒四人在取经的路上，由互相对抗到同心合力，成为无坚不摧的驱魔团队。吴亦凡还和梁朝伟、唐嫣合作动作片《欧洲攻略》，该片讲述江湖排名第一、第二的林先生(梁朝伟饰)和王小姐(唐嫣饰)亦敌亦友，二人与助手乐奇(吴亦凡饰)分别追踪盗走“上帝之手”地震飞弹的苏菲，不想却引出了欧洲黑帮、美国CIA、欧盟打击犯罪联盟特工们的全力搜捕的故事。吴亦凡2017年在电影方面有更大突破，他加盟好莱坞大片《极限特工3：终极回归》，与范·迪塞尔、甄子丹、妮娜·杜波夫等一众大明星搭档，为电影献唱主题曲《JUICE》。此外，他还参演吕克·贝松执导的科幻电影《星际特工：千星之城》，该片讲述一个发生在未来28世纪星际警察穿越时空的故事，影片有望2017年上映。'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7623081207275391"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0  = time()\n",
    "for i in range(0, 100):  \n",
    "    print (i, '*****************')\n",
    "    for t in get_abstract(df_check.iloc[i]['content']):  \n",
    "        print (t)\n",
    "        print ('--------------------')\n",
    "\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text summarization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "  def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "    \"\"\"\n",
    "     Initilize the text summarizer.\n",
    "     Words that have a frequency term lower than min_cut \n",
    "     or higer than max_cut will be ignored.\n",
    "    \"\"\"\n",
    "    self._min_cut = min_cut\n",
    "    self._max_cut = max_cut \n",
    "    self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "  def _compute_frequencies(self, word_sent):\n",
    "    \"\"\" \n",
    "      Compute the frequency of each of word.\n",
    "      Input: \n",
    "       word_sent, a list of sentences already tokenized.\n",
    "      Output: \n",
    "       freq, a dictionary where freq[w] is the frequency of w.\n",
    "    \"\"\"\n",
    "    freq = defaultdict(int)\n",
    "    for s in word_sent:\n",
    "      for word in s:\n",
    "        if word not in self._stopwords:\n",
    "          freq[word] += 1\n",
    "    # frequencies normalization and fitering\n",
    "    m = float(max(freq.values()))\n",
    "    for w in freq.keys():\n",
    "      freq[w] = freq[w]/m\n",
    "      if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "        del freq[w]\n",
    "    return freq\n",
    "\n",
    "  def summarize(self, text, n):\n",
    "    \"\"\"\n",
    "      Return a list of n sentences \n",
    "      which represent the summary of text.\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    assert n <= len(sents)\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "    self._freq = self._compute_frequencies(word_sent)\n",
    "    ranking = defaultdict(int)\n",
    "    for i,sent in enumerate(word_sent):\n",
    "      for w in sent:\n",
    "        if w in self._freq:\n",
    "          ranking[i] += self._freq[w]\n",
    "    sents_idx = self._rank(ranking, n)    \n",
    "    return [sents[j] for j in sents_idx]\n",
    "\n",
    "  def _rank(self, ranking, n):\n",
    "    \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "    return nlargest(n, ranking, key=ranking.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = FrequencySummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7959449291229248"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0  = time()\n",
    "for i in range(0, 100):  \n",
    "    print (i, '*****************')\n",
    "    for s in fs.summarize(df_check.iloc[i]['content'].decode('utf-8'), 2):\n",
    "        print (s)\n",
    "        print ('--------------------')\n",
    "\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
