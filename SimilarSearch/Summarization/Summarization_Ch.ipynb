{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/huizhou-final-v2-seg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://cul.anhuinews.com/system/2016/11/28/007...</td>\n",
       "      <td>anhuinews.com</td>\n",
       "      <td>洪建华夫妇的竹刻人生</td>\n",
       "      <td>谁能想到，洪建华夫妇这对一无学历、二无家传、三无师承的“三无大师”成了省级非物质文化遗产·徽...</td>\n",
       "      <td>谁 能 想到 ， 洪 建华 夫妇 这 对 一 无 学历 、 二 无 家传 、 三无 师承 的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://cul.anhuinews.com/system/2016/11/21/007...</td>\n",
       "      <td>anhuinews.com</td>\n",
       "      <td>徽州民歌传人操明花校园授艺</td>\n",
       "      <td>近日，省非物质文化遗产徽州民歌传承人操明花前往休宁县海阳二小、歙县城关小学和徽州区岩寺镇中心...</td>\n",
       "      <td>近日 ， 省非 物质 文化遗产 徽州 民歌 传承 人操 明花 前往 休宁县 海阳 二小 、 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://cul.anhuinews.com/system/2016/11/03/007...</td>\n",
       "      <td>anhuinews.com</td>\n",
       "      <td>黄山楹联漫谈</td>\n",
       "      <td>黄山楹联第一人当推明朝诗人余绍祉。他遍历祖国名山大川，见到无数奇峰怪石，却感到抵不上黄山峰林...</td>\n",
       "      <td>黄山 楹联 第一 人当 推 明朝 诗人 余绍祉 。 他 遍历 祖国 名山大川 ， 见到 无数...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://cul.anhuinews.com/system/2016/01/26/007...</td>\n",
       "      <td>anhuinews.com</td>\n",
       "      <td>徽派雕刻——徽州木雕艺术</td>\n",
       "      <td>徽州民居、家具木板和圆木雕刻艺术的简称。木雕在旧属徽州各县分布之广在全国屈指可数，宅院内的屏...</td>\n",
       "      <td>徽州 民居 、 家具 木板 和 圆木 雕刻 艺术 的 简称 。 木雕 在 旧属 徽州 各县 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://cul.anhuinews.com/system/2016/01/26/007...</td>\n",
       "      <td>anhuinews.com</td>\n",
       "      <td>徽派雕刻——徽州砖雕艺术</td>\n",
       "      <td>徽州砖雕徽州砖雕的造型，来源于汉画象砖。汉画象砖大都是模印砖坯刻画后入密烧制，嵌窃时再进行修...</td>\n",
       "      <td>徽州 砖雕 徽州 砖雕 的 造型 ， 来源于 汉 画象 砖 。 汉 画象 砖 大都 是 模印...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url         domain  \\\n",
       "0  http://cul.anhuinews.com/system/2016/11/28/007...  anhuinews.com   \n",
       "1  http://cul.anhuinews.com/system/2016/11/21/007...  anhuinews.com   \n",
       "2  http://cul.anhuinews.com/system/2016/11/03/007...  anhuinews.com   \n",
       "3  http://cul.anhuinews.com/system/2016/01/26/007...  anhuinews.com   \n",
       "4  http://cul.anhuinews.com/system/2016/01/26/007...  anhuinews.com   \n",
       "\n",
       "           title                                            content  \\\n",
       "0     洪建华夫妇的竹刻人生  谁能想到，洪建华夫妇这对一无学历、二无家传、三无师承的“三无大师”成了省级非物质文化遗产·徽...   \n",
       "1  徽州民歌传人操明花校园授艺  近日，省非物质文化遗产徽州民歌传承人操明花前往休宁县海阳二小、歙县城关小学和徽州区岩寺镇中心...   \n",
       "2         黄山楹联漫谈  黄山楹联第一人当推明朝诗人余绍祉。他遍历祖国名山大川，见到无数奇峰怪石，却感到抵不上黄山峰林...   \n",
       "3   徽派雕刻——徽州木雕艺术  徽州民居、家具木板和圆木雕刻艺术的简称。木雕在旧属徽州各县分布之广在全国屈指可数，宅院内的屏...   \n",
       "4   徽派雕刻——徽州砖雕艺术  徽州砖雕徽州砖雕的造型，来源于汉画象砖。汉画象砖大都是模印砖坯刻画后入密烧制，嵌窃时再进行修...   \n",
       "\n",
       "                                         content_seg  \n",
       "0  谁 能 想到 ， 洪 建华 夫妇 这 对 一 无 学历 、 二 无 家传 、 三无 师承 的...  \n",
       "1  近日 ， 省非 物质 文化遗产 徽州 民歌 传承 人操 明花 前往 休宁县 海阳 二小 、 ...  \n",
       "2  黄山 楹联 第一 人当 推 明朝 诗人 余绍祉 。 他 遍历 祖国 名山大川 ， 见到 无数...  \n",
       "3  徽州 民居 、 家具 木板 和 圆木 雕刻 艺术 的 简称 。 木雕 在 旧属 徽州 各县 ...  \n",
       "4  徽州 砖雕 徽州 砖雕 的 造型 ， 来源于 汉 画象 砖 。 汉 画象 砖 大都 是 模印...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['domain'] != 'huaxia.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='http://en.wikipedia.org/wiki/Automatic_summarization'\n",
    "parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk1 = df['content'].str.contains('徽州') \n",
    "msk2 = df['content'].str.contains('徽文化') \n",
    "msk = msk1 | msk2\n",
    "df_check = df[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_check.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple library and command line utility for extracting summary from HTML pages or plain texts. The package \n",
    "#also contains simple evaluation framework for text summaries. Implemented summarization methods:\n",
    "\n",
    "#    Luhn - heurestic method, reference\n",
    "#    Edmundson heurestic method with previous statistic research, reference\n",
    "#    Latent Semantic Analysis, LSA - one of the algorithm from http://scholar.google.com/citations?user=0fTuW_YAAAAJ&hl=en I think the author is using more advanced algorithms now. Steinberger, J. a JeĹľek, K. Using latent semantic an and summary evaluation. In In Proceedings ISIM ‘04. 2004. S. 93-100.\n",
    "#    LexRank - Unsupervised approach inspired by algorithms PageRank and HITS, reference\n",
    "#    TextRank - some sort of combination of a few resources that I found on the internet. I really don’t remember the sources. Probably Wikipedia and some papers in 1st page of Google :)\n",
    "#    SumBasic - Method that is often used as a baseline in the literature. Source: Read about SumBasic\n",
    "#    KL-Sum - Method that greedily adds sentences to a summary so long as it decreases the KL Divergence. Source: Read about KL-Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.219939947128296"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "for i in range(0, 100):  #8.591509103775024s\n",
    "    print (i, '------------------')\n",
    "    parser = HtmlParser.from_string(df_check.iloc[i]['content'], 'none', Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    #print (parser.document)\n",
    "    idx = 0\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            if (1): #idx != 0):\n",
    "                print(sentence)\n",
    "                print ('*********')\n",
    "            idx += 1\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence\n",
    "tr4s = TextRank4Sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.57102489471436"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time()\n",
    "from time import time\n",
    "t0 = time()\n",
    "for i in range(0, 100):  #11.573723077774048\n",
    "    print (i, '***********************')\n",
    "    tr4s.analyze(text=df_check.iloc[i]['content'], lower=True, source = 'all_filters')\n",
    "    for item in tr4s.get_key_sentences(num=3):\n",
    "        print(item.index, item.weight, item.sentence)  # index是语句在文本中位置，weight是权重\n",
    "        print ('------------------')\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf-8  \n",
    "import jieba  \n",
    "import networkx as nx  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer  \n",
    "  \n",
    "def cut_sentence(sentence):  \n",
    "    \"\"\" \n",
    "    分句 \n",
    "    :param sentence: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    if not isinstance(sentence, unicode):  \n",
    "        sentence = sentence.decode('utf-8')  \n",
    "    delimiters = frozenset(u'。！？')  \n",
    "    buf = []  \n",
    "    for ch in sentence:  \n",
    "        buf.append(ch)  \n",
    "        if delimiters.__contains__(ch):  \n",
    "            yield ''.join(buf)  \n",
    "            buf = []  \n",
    "    if buf:  \n",
    "        yield ''.join(buf)  \n",
    "  \n",
    "  \n",
    "def load_stopwords(path='/home/fhqplzj/data/orion/stopwords.txt'):  \n",
    "    \"\"\" \n",
    "    加载停用词 \n",
    "    :param path: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    with open(path) as f:  \n",
    "        stopwords = filter(lambda x: x, map(lambda x: x.strip().decode('utf-8'), f.readlines()))  \n",
    "    stopwords.extend([' ', '\\t', '\\n'])  \n",
    "    return frozenset(stopwords)  \n",
    "  \n",
    "def cut_words(sentence):  \n",
    "    \"\"\" \n",
    "    分词 \n",
    "    :param sentence: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    stopwords = load_stopwords()  \n",
    "    return filter(lambda x: not stopwords.__contains__(x), jieba.cut(sentence))  \n",
    "  \n",
    "def get_abstract(content, size=3):  \n",
    "    \"\"\" \n",
    "    利用textrank提取摘要 \n",
    "    :param content: \n",
    "    :param size: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    docs = list(cut_sentence(content))  \n",
    "    \n",
    "    #tfidf_model = TfidfVectorizer(tokenizer=jieba.cut, stop_words=load_stopwords())  \n",
    "    tfidf_model = TfidfVectorizer()  \n",
    "    tfidf_matrix = tfidf_model.fit_transform(docs)  \n",
    "    normalized_matrix = TfidfTransformer().fit_transform(tfidf_matrix)  \n",
    "    similarity = nx.from_scipy_sparse_matrix(normalized_matrix * normalized_matrix.T)  \n",
    "    scores = nx.pagerank(similarity)  \n",
    "    tops = sorted(scores.iteritems(), key=lambda x: x[1], reverse=True)  \n",
    "    size = min(size, len(docs))  \n",
    "    indices = map(lambda x: x[0], tops)[:size]  \n",
    "    return map(lambda idx: docs[idx], indices)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = u'要说现在当红的90后男星，那就不得不提鹿晗、吴亦凡、杨洋、张艺兴、黄子韬、陈学冬、刘昊然，2016年他们带来不少人气爆棚的影视剧。这些90后男星不仅有颜值、有才华，还够努力，2017年他们又有哪些傲娇的作品呢？到底谁会成为2017霸屏男神，让我们拭目以待吧。鹿晗2016年参演《盗墓笔记》、《长城》、《摆渡人》等多部电影，2017年他将重心转到了电视剧。他和古力娜扎主演的古装奇幻电视剧《择天记》将在湖南卫视暑期档播出，这是鹿晗个人的首部电视剧，也是其第一次出演古装题材。该剧改编自猫腻的同名网络小说，讲述在人妖魔共存的架空世界里，陈长生(鹿晗饰演)为了逆天改命，带着一纸婚书来到神都，结识了一群志同道合的小伙伴，在国教学院打开一片新天地。吴亦凡在2017年有更多的作品推出。周星驰监制、徐克执导的春节档《西游伏魔篇》，吴亦凡扮演“有史以来最帅的”唐僧。师徒四人在取经的路上，由互相对抗到同心合力，成为无坚不摧的驱魔团队。吴亦凡还和梁朝伟、唐嫣合作动作片《欧洲攻略》，该片讲述江湖排名第一、第二的林先生(梁朝伟饰)和王小姐(唐嫣饰)亦敌亦友，二人与助手乐奇(吴亦凡饰)分别追踪盗走“上帝之手”地震飞弹的苏菲，不想却引出了欧洲黑帮、美国CIA、欧盟打击犯罪联盟特工们的全力搜捕的故事。吴亦凡2017年在电影方面有更大突破，他加盟好莱坞大片《极限特工3：终极回归》，与范·迪塞尔、甄子丹、妮娜·杜波夫等一众大明星搭档，为电影献唱主题曲《JUICE》。此外，他还参演吕克·贝松执导的科幻电影《星际特工：千星之城》，该片讲述一个发生在未来28世纪星际警察穿越时空的故事，影片有望2017年上映。'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909038543701172"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0  = time()\n",
    "for i in range(0, 100):  \n",
    "    print (i, '*****************')\n",
    "    for t in get_abstract(df_check.iloc[i]['content']):  \n",
    "        print (t)\n",
    "        print ('--------------------')\n",
    "\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text summarization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "  def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "    \"\"\"\n",
    "     Initilize the text summarizer.\n",
    "     Words that have a frequency term lower than min_cut \n",
    "     or higer than max_cut will be ignored.\n",
    "    \"\"\"\n",
    "    self._min_cut = min_cut\n",
    "    self._max_cut = max_cut \n",
    "    self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "  def _compute_frequencies(self, word_sent):\n",
    "    \"\"\" \n",
    "      Compute the frequency of each of word.\n",
    "      Input: \n",
    "       word_sent, a list of sentences already tokenized.\n",
    "      Output: \n",
    "       freq, a dictionary where freq[w] is the frequency of w.\n",
    "    \"\"\"\n",
    "    freq = defaultdict(int)\n",
    "    for s in word_sent:\n",
    "      for word in s:\n",
    "        if word not in self._stopwords:\n",
    "          freq[word] += 1\n",
    "    # frequencies normalization and fitering\n",
    "    m = float(max(freq.values()))\n",
    "    for w in freq.keys():\n",
    "      freq[w] = freq[w]/m\n",
    "      if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "        del freq[w]\n",
    "    return freq\n",
    "\n",
    "  def summarize(self, text, n):\n",
    "    \"\"\"\n",
    "      Return a list of n sentences \n",
    "      which represent the summary of text.\n",
    "    \"\"\"\n",
    "    sents = sent_tokenize(text)\n",
    "    assert n <= len(sents)\n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "    self._freq = self._compute_frequencies(word_sent)\n",
    "    ranking = defaultdict(int)\n",
    "    for i,sent in enumerate(word_sent):\n",
    "      for w in sent:\n",
    "        if w in self._freq:\n",
    "          ranking[i] += self._freq[w]\n",
    "    sents_idx = self._rank(ranking, n)    \n",
    "    return [sents[j] for j in sents_idx]\n",
    "\n",
    "  def _rank(self, ranking, n):\n",
    "    \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "    return nlargest(n, ranking, key=ranking.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = FrequencySummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b6e829f13d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*****************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3de9a807b5e0>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, text, n)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[1;32m     38\u001b[0m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mword_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0  = time()\n",
    "for i in range(0, 100):  \n",
    "    print (i, '*****************')\n",
    "    for s in fs.summarize(df_check.iloc[i]['content'].decode('utf-8'), 2):\n",
    "        print (s)\n",
    "        print ('--------------------')\n",
    "\n",
    "time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
