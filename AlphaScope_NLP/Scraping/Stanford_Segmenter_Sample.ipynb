{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Natural Language Toolkit: Interface to the Stanford Chinese Segmenter\n",
    "#\n",
    "# Copyright (C) 2001-2014 NLTK Project\n",
    "# Author: 52nlp <52nlpcn@gmail.com>\n",
    "#\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "from subprocess import PIPE\n",
    "\n",
    "from nltk import compat\n",
    "from nltk.internals import find_jar, config_java, java, _java_options\n",
    "\n",
    "from nltk.tokenize.api import TokenizerI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StanfordSegmenter(TokenizerI):\n",
    "    r\"\"\"\n",
    "    Interface to the Stanford Segmenter\n",
    "\n",
    "    >>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "    >>> segmenter = StanfordSegmenter(path_to_jar=\"stanford-segmenter-3.4.1.jar\", path_to_sihan_corpora_dict=\"./data\", path_to_model=\"./data/pku.gz\", path_to_dict=\"./data/dict-chris6.ser.gz\")\n",
    "    >>> sentence = u\"这是斯坦福中文分词器测试\"\n",
    "    >>> segmenter.segment(sentence)\n",
    "    >>> u'\\u8fd9 \\u662f \\u65af\\u5766\\u798f \\u4e2d\\u6587 \\u5206\\u8bcd\\u5668 \\u6d4b\\u8bd5\\n'\n",
    "    >>> segmenter.segment_file(\"test.simp.utf8\")\n",
    "    >>> u'\\u9762\\u5bf9 \\u65b0 \\u4e16\\u7eaa \\uff0c \\u4e16\\u754c \\u5404\\u56fd ...\n",
    "    \"\"\"\n",
    "\n",
    "    _JAR = 'stanford-segmenter.jar'\n",
    "\n",
    "    def __init__(self, path_to_jar=None,\n",
    "            path_to_sihan_corpora_dict=None,\n",
    "            path_to_model=None, path_to_dict=None,\n",
    "            encoding='UTF-8', options=None,\n",
    "            verbose=False, java_options='-mx2g'):\n",
    "        self._stanford_jar = find_jar(\n",
    "            self._JAR, path_to_jar,\n",
    "            env_vars=('STANFORD_SEGMENTER',),\n",
    "            searchpath=(),\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self._sihan_corpora_dict = path_to_sihan_corpora_dict\n",
    "        self._model = path_to_model\n",
    "        self._dict = path_to_dict\n",
    "\n",
    "        self._encoding = encoding\n",
    "        self.java_options = java_options\n",
    "        options = {} if options is None else options\n",
    "        self._options_cmd = ','.join('{0}={1}'.format(key, json.dumps(val)) for key, val in options.items())\n",
    "\n",
    "    def segment_file(self, input_file_path):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            'edu.stanford.nlp.ie.crf.CRFClassifier',\n",
    "            '-sighanCorporaDict', self._sihan_corpora_dict,\n",
    "            '-textFile', input_file_path,\n",
    "            '-sighanPostProcessing', 'true',\n",
    "            '-keepAllWhitespaces', 'false',\n",
    "            '-loadClassifier', self._model,\n",
    "            '-serDictionary', self._dict\n",
    "        ]\n",
    "\n",
    "        stdout = self._execute(cmd)\n",
    "\n",
    "        return stdout\n",
    "\n",
    "    def segment(self, tokens):\n",
    "        return self.segment_sents([tokens])\n",
    "\n",
    "    def segment_sents(self, sentences):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        encoding = self._encoding\n",
    "        # Create a temporary input file\n",
    "        _input_fh, self._input_file_path = tempfile.mkstemp(text=True)\n",
    "\n",
    "        # Write the actural sentences to the temporary input file\n",
    "        _input_fh = os.fdopen(_input_fh, 'wb')\n",
    "        _input = '\\n'.join((' '.join(x) for x in sentences))\n",
    "        if isinstance(_input, compat.text_type) and encoding:\n",
    "            _input = _input.encode(encoding)\n",
    "        _input_fh.write(_input)\n",
    "        _input_fh.close()\n",
    "\n",
    "        cmd = [\n",
    "            'edu.stanford.nlp.ie.crf.CRFClassifier',\n",
    "            '-sighanCorporaDict', self._sihan_corpora_dict,\n",
    "            '-textFile', self._input_file_path,\n",
    "            '-sighanPostProcessing', 'true',\n",
    "            '-keepAllWhitespaces', 'false',\n",
    "            '-loadClassifier', self._model,\n",
    "            '-serDictionary', self._dict\n",
    "        ]\n",
    "\n",
    "        stdout = self._execute(cmd)\n",
    "\n",
    "        # Delete the temporary file\n",
    "        os.unlink(self._input_file_path)\n",
    "\n",
    "        return stdout\n",
    "\n",
    "    def _execute(self, cmd, verbose=False):\n",
    "        encoding = self._encoding\n",
    "        cmd.extend(['-inputEncoding', encoding])\n",
    "        _options_cmd = self._options_cmd\n",
    "        if _options_cmd:\n",
    "            cmd.extend(['-options', self._options_cmd])\n",
    "\n",
    "        default_options = ' '.join(_java_options)\n",
    "\n",
    "        # Configure java.\n",
    "        config_java(options=self.java_options, verbose=verbose)\n",
    "\n",
    "        stdout, _stderr = java(cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)\n",
    "        stdout = stdout.decode(encoding)\n",
    "\n",
    "        # Return java configurations to their default values.\n",
    "        config_java(options=default_options, verbose=False)\n",
    "\n",
    "        return stdout\n",
    "\n",
    "def setup_module(module):\n",
    "    from nose import SkipTest\n",
    "\n",
    "    try:\n",
    "        StanfordSegmenter()\n",
    "    except LookupError:\n",
    "        raise SkipTest('doctests from nltk.tokenize.stanford_segmenter are skipped because the stanford segmenter jar doesn\\'t exist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
