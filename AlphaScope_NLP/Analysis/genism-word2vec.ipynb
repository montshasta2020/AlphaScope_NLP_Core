{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('nutch-crawl-01-24-2016-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88417 entries, 0 to 88416\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0    88417 non-null int64\n",
      "tstamp        88417 non-null object\n",
      "domain        88417 non-null object\n",
      "url           88417 non-null object\n",
      "content       88417 non-null object\n",
      "title         87117 non-null object\n",
      "doc_len       88417 non-null int64\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['doc_len'] = df['content'].str.len()\n",
    "df_dropped = df[df['doc_len'] > 100]\n",
    "df_final = df_dropped[['tstamp', 'domain', 'url', 'title', 'content', 'doc_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df_final.loc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1978 entries, 0 to 2000\n",
      "Data columns (total 6 columns):\n",
      "tstamp     1978 non-null object\n",
      "domain     1978 non-null object\n",
      "url        1978 non-null object\n",
      "title      1943 non-null object\n",
      "content    1978 non-null object\n",
      "doc_len    1978 non-null int64\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 108.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('2016-ds-test_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = df_test['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(doc):\n",
    "    lowers = doc.lower()\n",
    "    #no_punctuation = lowers.translate(None, string.punctuation)\n",
    "\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    no_punctuation_decode = no_punctuation.decode('ascii', errors='ignore')\n",
    "\n",
    "    tokens = nltk.word_tokenize(no_punctuation_decode)\n",
    "    \n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english') and len(w) > 1]\n",
    "    \n",
    "    #tokens_no_stopwords = [token for token in tokens if token not in stopwords.word('english')] \n",
    "    #return tokens_no_stopwords\n",
    "    \n",
    "#    print (len(filtered))\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1978\n"
     ]
    }
   ],
   "source": [
    "sentences = [get_tokens(article) for article in articles]\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " model = word2vec.Word2Vec(sentences, size=2000, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'big', 0.5052622556686401),\n",
       " (u'systems', 0.398093581199646),\n",
       " (u'mining', 0.38064810633659363),\n",
       " (u'warehouse', 0.3699193000793457),\n",
       " (u'scientist', 0.3676573634147644),\n",
       " (u'analytics', 0.36224448680877686),\n",
       " (u'borough', 0.36033928394317627),\n",
       " (u'integration', 0.36000216007232666),\n",
       " (u'searchproblems', 0.35709285736083984),\n",
       " (u'boroughs', 0.35334333777427673),\n",
       " (u'citymapper', 0.3419044613838196),\n",
       " (u'semanticweb', 0.3345053195953369),\n",
       " (u'shadow', 0.32012492418289185),\n",
       " (u'visualization', 0.3149222731590271),\n",
       " (u'marts', 0.3127674162387848),\n",
       " (u'various', 0.3126369118690491),\n",
       " (u'quality', 0.31200695037841797),\n",
       " (u'nyc', 0.31011709570884705),\n",
       " (u'datareview', 0.305535227060318),\n",
       " (u'signpost', 0.3012754023075104)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['data', 'science'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-06c3a89b73e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
