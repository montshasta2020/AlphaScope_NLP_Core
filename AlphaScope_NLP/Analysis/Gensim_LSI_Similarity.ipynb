{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding=gbk-*-\n",
    " \n",
    "\"\"\"\n",
    "     原始数据，用于建立模型\n",
    "\"\"\"\n",
    "#缩水版的courses，实际数据的格式应该为 课程名\\t课程简介\\t课程详情，并已去除html等干扰因素\n",
    "courses = [           \n",
    "            u'Writing II: Rhetorical Composing',\n",
    "            u'Genetics and Society: A Course for Educators',\n",
    "            u'General Game Playing',\n",
    "            u'Genes and the Human Condition (From Behavior to Biotechnology)',\n",
    "            u'A Brief History of Humankind',\n",
    "            u'New Models of Business in Society',\n",
    "            u'Analyse Numrique pour Ingnieurs',\n",
    "            u'Evolution: A Course for Educators',\n",
    "            u'Coding the Matrix: Linear Algebra through Computer Science Applications',\n",
    "            u'The Dynamic Earth: A Course for Educators',\n",
    "            u'Tiny Wings\\tYou have always dreamed of flying - but your wings are tiny. Luckily the world is full of beautiful hills. Use the hills as jumps - slide down, flap your wings and fly! At least for a moment - until this annoying gravity brings you back down to earth. But the next hill is waiting for you already. Watch out for the night and fly as fast as you can. ',\n",
    "            u'Angry Birds Free',\n",
    "            u'没有\\它很相似',\n",
    "            u'没有\\t它很相似',\n",
    "            u'没有\\t他很相似',\n",
    "            u'没有\\t他不很相似',\n",
    "            u'没有',\n",
    "            u'可以没有',\n",
    "            u'也没有',\n",
    "            u'有没有也不管',\n",
    "            u'Angry Birds Stella',\n",
    "            u'Flappy Wings - FREE\\tFly into freedom!A parody of the #1 smash hit game!',\n",
    "            u'没有一个',\n",
    "            u'没有一个2',\n",
    "           ]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_no_dup = pd.read_csv('../data/classifier/bosonNLP/bosonNLP-classifier-tuned.csv')\n",
    "df = pd.read_csv('/Users/nhu2000/Desktop/huizhou/data/huiwenhua/huizhou_web_content_slim_v2-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = df[df['domain'] != 'wuyuan168.com']\n",
    "df_final = df_final.dropna(subset =['key_words'])\n",
    "df_final = df_final[df_final['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = df['combined'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>segment</th>\n",
       "      <th>key_words</th>\n",
       "      <th>summary</th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.hzwh.com/article.asp?id=1316</td>\n",
       "      <td>hzwh.com</td>\n",
       "      <td>用心灵感悟徽文化的博大精深</td>\n",
       "      <td>用 心灵 感悟 文化 徽文化 的 博大 精深 博大精深</td>\n",
       "      <td>文化 文脉 古村落 万善 古民居 可视性 亭榭 统一性 儒家思想 无处不在 昭示 明清两代 ...</td>\n",
       "      <td>宋代兴起的理学是儒家思想的一个流派，明清两代达到巅峰，与中国几千年的文化具有统一性，又呈现多...</td>\n",
       "      <td>徽州自古人文荟萃，丰厚的人文遗产，几乎涵盖了哲学、社会、教育、艺术、宗教、建筑、医...</td>\n",
       "      <td>1</td>\n",
       "      <td>用 心灵 感悟 文化 徽文化 博大 精深 博大精深文化 文脉 古村落 万善 古民居 可视性 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>www.hzwh.com/article.asp?id=1203</td>\n",
       "      <td>hzwh.com</td>\n",
       "      <td>清末民初徽商兴办实业教育</td>\n",
       "      <td>清末 民初 徽商 兴办 实业 教育</td>\n",
       "      <td>徽州 绩溪 创办 茶商 蚕桑 学堂 农业 朱里 邹等 戴琴泉 吴俊德 六县 周协恭 传技 汪...</td>\n",
       "      <td>清光绪末年，休宁茶商戴琴泉等在屯溪朱里成立农业公司，栽桑养蚕，同时创办了初等农业学堂，培养蚕...</td>\n",
       "      <td>清末，称雄商界几百年的徽商，已是日薄西山，渐呈衰败。当时徽商中的部分有识之士已经看到...</td>\n",
       "      <td>1</td>\n",
       "      <td>清末 民初 徽商 兴办 实业 教育徽州 绩溪 创办 茶商 蚕桑 学堂 农业 朱里 邹等 戴琴...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.hzwh.com/article.asp?id=907</td>\n",
       "      <td>hzwh.com</td>\n",
       "      <td>新安画派的绘画风格</td>\n",
       "      <td>新安 画派 的 绘画 风格</td>\n",
       "      <td>新安 画派 山水 画家 创作 观察 学养 篆刻家 范本 熏陶 徽州 灵感 大好 汲取 素材 ...</td>\n",
       "      <td>新安大好山水为当地画家提供了很好的范本，更是他们从中汲取创作灵感的源泉 新安画派注重对自然的...</td>\n",
       "      <td>一、绘画技法上崇尚元四家，推倪瓒为宗师。构图上讲求疏密相衬、山水相生，动与静、远与近...</td>\n",
       "      <td>1</td>\n",
       "      <td>新安 画派 绘画 风格新安 画派 山水 画家 创作 观察 学养 篆刻家 范本 熏陶 徽州 灵...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.xinhs.cn/article.asp?id=38700</td>\n",
       "      <td>xinhs.cn</td>\n",
       "      <td>安徽大学研究生在茗洲村开展徽州人生仪礼课题调查活动</td>\n",
       "      <td>安徽 大学 安徽大学 研究 研究生 在 茗 洲 村 开展 徽州 人生 仪礼 课题 调查 活动</td>\n",
       "      <td>徽州 村落 礼仪 了解 习俗 婚丧 吴氏家典 学生 吴氏 婚庆 礼教 文化 座谈 仪礼 民居...</td>\n",
       "      <td>实地考察中，民俗专业的学生与村中  多岁的老人座谈，详细地了解了这个古老村落的婚庆、丧葬习俗...</td>\n",
       "      <td>实地考察中，民俗专业的学生与村中80多岁的老人座谈，详细地了解了这个古老村落...</td>\n",
       "      <td>1</td>\n",
       "      <td>安徽 大学 安徽大学 研究 研究生 茗 洲 村 开展 徽州 人生 仪礼 课题 调查 活动徽州...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.xinhs.cn/article.asp?id=80495</td>\n",
       "      <td>xinhs.cn</td>\n",
       "      <td>新安地理练江上的古歙三桥</td>\n",
       "      <td>新安 地理 练江 上 的 古 歙 三桥</td>\n",
       "      <td>太平桥 练江 渔梁坝 三桥 紫阳 浮桥 横跨 惊涛拍岸 冲坏 卧波 刘炳 石拱桥 端平 庆丰...</td>\n",
       "      <td>其中著名的要算建于明朝的太平桥、万年桥和紫阳桥了，三座桥又合称古歙三桥，如长虹卧波一样，横跨...</td>\n",
       "      <td>春天的新安江，美丽如画。站在岸边，我们不住地轻声而叹。歙县县城一带的新安江，...</td>\n",
       "      <td>1</td>\n",
       "      <td>新安 地理 练江 古 歙 三桥太平桥 练江 渔梁坝 三桥 紫阳 浮桥 横跨 惊涛拍岸 冲坏 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        url    domain  \\\n",
       "1          www.hzwh.com/article.asp?id=1316  hzwh.com   \n",
       "2          www.hzwh.com/article.asp?id=1203  hzwh.com   \n",
       "3           www.hzwh.com/article.asp?id=907  hzwh.com   \n",
       "4  http://www.xinhs.cn/article.asp?id=38700  xinhs.cn   \n",
       "5  http://www.xinhs.cn/article.asp?id=80495  xinhs.cn   \n",
       "\n",
       "                       title                                         segment  \\\n",
       "1              用心灵感悟徽文化的博大精深                     用 心灵 感悟 文化 徽文化 的 博大 精深 博大精深   \n",
       "2               清末民初徽商兴办实业教育                               清末 民初 徽商 兴办 实业 教育   \n",
       "3                  新安画派的绘画风格                                   新安 画派 的 绘画 风格   \n",
       "4  安徽大学研究生在茗洲村开展徽州人生仪礼课题调查活动  安徽 大学 安徽大学 研究 研究生 在 茗 洲 村 开展 徽州 人生 仪礼 课题 调查 活动   \n",
       "5               新安地理练江上的古歙三桥                             新安 地理 练江 上 的 古 歙 三桥   \n",
       "\n",
       "                                           key_words  \\\n",
       "1  文化 文脉 古村落 万善 古民居 可视性 亭榭 统一性 儒家思想 无处不在 昭示 明清两代 ...   \n",
       "2  徽州 绩溪 创办 茶商 蚕桑 学堂 农业 朱里 邹等 戴琴泉 吴俊德 六县 周协恭 传技 汪...   \n",
       "3  新安 画派 山水 画家 创作 观察 学养 篆刻家 范本 熏陶 徽州 灵感 大好 汲取 素材 ...   \n",
       "4  徽州 村落 礼仪 了解 习俗 婚丧 吴氏家典 学生 吴氏 婚庆 礼教 文化 座谈 仪礼 民居...   \n",
       "5  太平桥 练江 渔梁坝 三桥 紫阳 浮桥 横跨 惊涛拍岸 冲坏 卧波 刘炳 石拱桥 端平 庆丰...   \n",
       "\n",
       "                                             summary  \\\n",
       "1  宋代兴起的理学是儒家思想的一个流派，明清两代达到巅峰，与中国几千年的文化具有统一性，又呈现多...   \n",
       "2  清光绪末年，休宁茶商戴琴泉等在屯溪朱里成立农业公司，栽桑养蚕，同时创办了初等农业学堂，培养蚕...   \n",
       "3  新安大好山水为当地画家提供了很好的范本，更是他们从中汲取创作灵感的源泉 新安画派注重对自然的...   \n",
       "4  实地考察中，民俗专业的学生与村中  多岁的老人座谈，详细地了解了这个古老村落的婚庆、丧葬习俗...   \n",
       "5  其中著名的要算建于明朝的太平桥、万年桥和紫阳桥了，三座桥又合称古歙三桥，如长虹卧波一样，横跨...   \n",
       "\n",
       "                                                body  target  \\\n",
       "1       徽州自古人文荟萃，丰厚的人文遗产，几乎涵盖了哲学、社会、教育、艺术、宗教、建筑、医...       1   \n",
       "2      清末，称雄商界几百年的徽商，已是日薄西山，渐呈衰败。当时徽商中的部分有识之士已经看到...       1   \n",
       "3      一、绘画技法上崇尚元四家，推倪瓒为宗师。构图上讲求疏密相衬、山水相生，动与静、远与近...       1   \n",
       "4          实地考察中，民俗专业的学生与村中80多岁的老人座谈，详细地了解了这个古老村落...       1   \n",
       "5          春天的新安江，美丽如画。站在岸边，我们不住地轻声而叹。歙县县城一带的新安江，...       1   \n",
       "\n",
       "                                            combined  \n",
       "1  用 心灵 感悟 文化 徽文化 博大 精深 博大精深文化 文脉 古村落 万善 古民居 可视性 ...  \n",
       "2  清末 民初 徽商 兴办 实业 教育徽州 绩溪 创办 茶商 蚕桑 学堂 农业 朱里 邹等 戴琴...  \n",
       "3  新安 画派 绘画 风格新安 画派 山水 画家 创作 观察 学养 篆刻家 范本 熏陶 徽州 灵...  \n",
       "4  安徽 大学 安徽大学 研究 研究生 茗 洲 村 开展 徽州 人生 仪礼 课题 调查 活动徽州...  \n",
       "5  新安 地理 练江 古 歙 三桥太平桥 练江 渔梁坝 三桥 紫阳 浮桥 横跨 惊涛拍岸 冲坏 ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courses = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "courses_name = courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    预处理(easy_install nltk)\n",
    "\"\"\"\n",
    "def pre_process_cn(courses, low_freq_filter = True):\n",
    "    \"\"\"\n",
    "     简化的 中文+英文 预处理\n",
    "        1.去掉停用词\n",
    "        2.去掉标点符号\n",
    "        3.处理为词干\n",
    "        4.去掉低频词\n",
    " \n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    import jieba.analyse\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    texts_tokenized = []\n",
    "    for document in courses:\n",
    "        texts_tokenized_tmp = []\n",
    "        for word in word_tokenize(document):\n",
    "            texts_tokenized_tmp += jieba.analyse.extract_tags(word,10)\n",
    "        texts_tokenized.append(texts_tokenized_tmp)   \n",
    "    \n",
    "    texts_filtered_stopwords = texts_tokenized\n",
    " \n",
    "    #去除标点符号\n",
    "    english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "    texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords]\n",
    " \n",
    "    #词干化\n",
    "    from nltk.stem.lancaster import LancasterStemmer\n",
    "    st = LancasterStemmer()\n",
    "    texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]\n",
    "    \n",
    "    #去除过低频词\n",
    "    if low_freq_filter:\n",
    "        all_stems = sum(texts_stemmed, [])\n",
    "        stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
    "        texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]\n",
    "    else:\n",
    "        texts = texts_stemmed\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d7adcdcb47cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlib_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_cn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5b3049cd4bbe>\u001b[0m in \u001b[0;36mpre_process_cn\u001b[0;34m(courses, low_freq_filter)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcourses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtexts_tokenized_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtexts_tokenized_tmp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtexts_tokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_tokenized_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdesigned\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwork\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mat\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_word_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#starting quotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'^\\\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'``'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(``)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr' \\1 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([ (\\[{<])\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'\\1 `` '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "lib_texts = pre_process_cn(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"\n",
    "    引入gensim，正式开始处理(easy_install gensim)\n",
    "\"\"\"\n",
    " \n",
    "def train_by_lsi(lib_texts):\n",
    "    \"\"\"\n",
    "        通过LSI模型的训练\n",
    "    \"\"\"\n",
    "    from gensim import corpora, models, similarities\n",
    " \n",
    "    #为了能看到过程日志\n",
    "    #import logging\n",
    "    #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "    dictionary = corpora.Dictionary(lib_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in lib_texts]     #doc2bow(): 将collection words 转为词袋，用两元组(word_id, word_frequency)表示\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    " \n",
    "    #拍脑袋的：训练topic数量为10的LSI模型\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20)\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])     # index 是 gensim.similarities.docsim.MatrixSimilarity 实例\n",
    "    \n",
    "    return (index, dictionary, lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "    \n",
    "    \n",
    "#库建立完成 -- 这部分可能数据很大，可以预先处理好，存储起来\n",
    "#(index,dictionary,lsi) = train_by_lsi(lib_texts)\n",
    "(index,dictionary,lsi) = train_by_lsi(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "fname1 = '../pkl/lsi_model.pkl'\n",
    "fname2 = '../pkl/lsi_index.pkl'\n",
    "fname3 = '../pkl/lsi_dictionary.pkl'\n",
    "with open(fname1, 'w') as f:\n",
    "    pickle.dump(lsi, f)\n",
    "    \n",
    "with open(fname2, 'w') as f:\n",
    "    pickle.dump(index, f)\n",
    "\n",
    "with open(fname3, 'w') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fname1) as f:\n",
    "    lsi = pickle.load(f)\n",
    "\n",
    "with open(fname2) as f:\n",
    "    index = pickle.load(f)\n",
    "\n",
    "with open(fname3) as f:\n",
    "    dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(index), len(dictionary), len(ml_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "#要处理的对象登场\n",
    "target_courses = u'徽文化'\n",
    "target_text = pre_process_cn(target_courses, low_freq_filter=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"\n",
    "对具体对象相似度匹配\n",
    "\"\"\"\n",
    " \n",
    "#选择一个基准数据\n",
    "#ml_course = df_final.iloc[3703]['segment'].split()\n",
    "ml_course = ['徽文化']\n",
    "\n",
    " \n",
    "#词袋处理\n",
    "ml_bow = dictionary.doc2bow(ml_course)  \n",
    " \n",
    "#在上面选择的模型数据 lsi 中，计算其他数据与其的相似度\n",
    "ml_lsi = lsi[ml_bow]     #ml_lsi 形式如 (topic_id, topic_value)\n",
    "\n",
    "print ml_lsi\n",
    "\n",
    "sims = index[ml_lsi]     #sims 是最终结果了， index[xxx] 调用内置方法 __getitem__() 来计算ml_lsi\n",
    " \n",
    "#排序，为输出方便\n",
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "tot = 5\n",
    "\n",
    "labels, sim_values = zip(*sort_sims[0:tot])\n",
    "\n",
    "print 'reference doc ---', labels[0], df_final.iloc[labels[0]]['title']\n",
    "\n",
    "for i in range(1, tot):\n",
    "    print labels[i], df_final.iloc[labels[i]]['title']\n",
    "    print labels[i], df_final.iloc[labels[i]]['url']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#查看结果\n",
    "#print sort_sims[0:10]   #看下前10个最相似的，第一个是基准数据自身\n",
    "#print courses_name[sort_sims[1][0]]   #看下实际最相似的数据叫什么\n",
    "#print courses_name[sort_sims[2][0]]   #看下实际最相似的数据叫什么\n",
    "#print courses_name[sort_sims[3][0]]   #看下实际最相似的数据叫什么\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'LsiModel' has no attribute 'get_similarities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3dc6efc746be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_lsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'LsiModel' has no attribute 'get_similarities'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "ret = models.LsiModel.get_similarities(ml_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ml_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<4979 docs, 20 features>\n"
     ]
    }
   ],
   "source": [
    "print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = lsi.show_topics(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 文化 吸附 身处 手艺人 手艺 道理 感受 六年 清光绪 嘉定 自宋 文武 徽州 状元 十年 休宁 徽派 保护 徽文化 建筑\n",
      "1 文化 道理 六年 吸附 手艺 身处 清光绪 嘉定 文武 手艺人 自宋 十年 感受 状元 保护 徽州 徽文化 徽派 黄山 建筑\n",
      "2 建筑 徽派 古建筑 民居 研究 雕刻 文化 古民居 古建 木雕 徽学 艺术 古村落 砖雕 保护 学术 装饰 徽商 新安 西递\n",
      "3 保护 徽派 徽商 新安 旅游 古村落 生态 古民居 黄山 利用 资料 艺术 黄山市 网上 网上家园 古建筑 本站 若有 管理员 侵权\n",
      "4 文化 研究 保护 徽学 歙县 本站 若有 管理员 侵权 网上 来源于 网上家园 资料 网络 徽商 绩溪 休宁 生态 联系 黟县\n",
      "5 徽学 研究 建筑 若有 管理员 本站 侵权 来源于 明清 网络 网上 网上家园 社会 徽商 联系 文书 安徽 未经许可 资料 随意\n",
      "6 徽商 协会会员 桐城人 年生 报业 甘臻 研究生 现为 研究 徽学 安徽省 新安 木雕 大会 雕刻 七章 第七章 经营 旅游 江淮\n",
      "7 徽商 协会会员 桐城人 年生 甘臻 报业 研究生 现为 木雕 黟县 第七章 江淮 西递 新安 安徽省 七章 第七 艺术 雕刻 明清\n",
      "8 徽商 休宁 新安 徽文化 休宁县 木雕 婺源 雕刻 黟县 故园 传承 屯溪 大会 文化 古村落 徽剧 医学 西递 歙县 祁门\n",
      "9 新安 绩溪 医学 画派 故园 木雕 胡适 徽商 活动 徽州 民俗 雕刻 周年 汪华 汪公 休宁 学术 保护 汪王 绩溪县\n",
      "10 绩溪 休宁 故园 黄山 祁门 万安 保护 休宁县 徽墨 旅游 生态 徽派 徽文化 公告 文化遗产 制作 罗盘 国家级 绩溪县 大会\n",
      "11 绩溪 徽菜 徽剧 黄山 休宁 木雕 休宁县 屯溪 雕刻 黄山市 歙县 万安 新安 徽文化 老街 古村落 罗盘 胡适 民俗 徽班\n",
      "12 黟县 屯溪 老街 西递 旅游 祁门 宏村 建筑 胡适 徽商 绩溪 故园 保护 木雕 古建筑 学术 周年 祁门县 徽剧 屯溪区\n",
      "13 徽剧 绩溪 徽班 徽菜 徽文化 京剧 徽派 旅游 歙县 故园 黄山 休宁 文化遗产 研究 胡适 四喜 西递 建筑 戏曲 进京\n",
      "14 徽剧 新安 保护 徽商 徽班 中国 徽学 故园 生态 屯溪 画派 历史 老街 黄山 文书 研究 医学 京剧 休宁县 徽菜\n",
      "15 歙县 民俗 黟县 绩溪 胡适 汪公 汪华 汪王 西递 建筑 宏村 木雕 新安 古城 文化 雕刻 抬 黄山 特色 宗族\n",
      "16 屯溪 祁门 黄山 黟县 老街 西递 祁门县 古建筑 徽墨 徽文化 宏村 文化 古建 祁红 大会 博物馆 建筑 徽剧 休宁 医学\n",
      "17 祁门 故园 祁门县 歙县 木雕 徽派 摄影 祁红 雕刻 文化遗产 中国 屯溪 制作 公告 建筑 印象 新春 物质 宣纸 汪华\n",
      "18 徽学 歙县 朱熹 文化 文书 理学 研究 徽墨 胡适 休宁县 徽派 万安 休宁 思想 传统 徽菜 屯溪 罗盘 中国 故园\n",
      "19 歙县 学术 故园 胡适 徽菜 中国 旅游 研讨会 新安 研讨 朱熹 戴震 绩溪 徽剧 新春 休宁 文化 医学 大会 徽文化\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = '\\\"(.*?)\\\"'\n",
    "for i in range(0, 20):\n",
    "    t = ret[i][1]\n",
    "    s = re.findall('\"([^\"]*)\"', t)\n",
    "    #print t\n",
    "    print i, ' '.join(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
